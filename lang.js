// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
import {keyword} from "./tokenizer.js"
export const parser = LRParser.deserialize({
  version: 14,
  states: "!WQYQROOO!QQRO'#ClOOQQ'#Cq'#CqOOQQ'#Cm'#CmQYQROOOOQQ,59W,59WO!XQRO,59WOOQQ-E6k-E6kOOQQ1G.r1G.r",
  stateData: "!`~OdOSYOS~OPQOQQORQOSQOTQOUQOVQOWQOXQO[QO]QO_PO~O^TO~PYO^WO~PYO",
  goto: "}fPPPPPPPPPPPPPPPPgmPPPwXQOPSUQSOQUPTVSUXROPSU",
  nodeNames: "âš  Directives Commands Extensions Reporters TurtleVars PatchVars LinkVars Constants Unsupported LineComment Program Numeric String ] [ Application",
  maxTerm: 21,
  nodeProps: [
    ["openedBy", 14,"["],
    ["closedBy", 15,"]"]
  ],
  skippedNodes: [0,10],
  repeatNodeCount: 1,
  tokenData: "#h~RXXYnYZn]^npqnrs!P!Q![!n!]!^#R!}#O#^#P#Q#c~sSd~XYnYZn]^npqn~!STOr!Prs!cs#O!P#O#P!h#P~!P~!hO]~~!kPO~!P~!sQ[~!O!P!y!Q![!n~#OP[~!Q![!y~#WQY~OY#RZ~#R~#cO_~~#hO^~",
  tokenizers: [0, keyword],
  topRules: {"Program":[0,11]},
  tokenPrec: 0
})
